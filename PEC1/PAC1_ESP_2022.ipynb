{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Idffb0thnM1H"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M1.304 · Inteligencia Artificial Avanzada / M0.539 · Inteligencia Artificial</p>\n",
    "<p style=\"margin: 0; text-align:right;\">MU Ingeniería Informática / MU Ingeniería Computacional y Matemática</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wXip_npz09_"
   },
   "source": [
    "## PEC 1: SISTEMAS RECOMENDADORES\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfCMCxAZTXtZ"
   },
   "source": [
    "### Presentación\n",
    "\n",
    "En esta PEC trabajaréis con sistemas recomendadores, tanto a bajo nivel (programando funciones y matrices de similitud) como a un nivel más alto (mediante la biblioteca Surprise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6UhmuehTb8G"
   },
   "source": [
    "### Competencias\n",
    "\n",
    "En este enunciado se trabajan las siguientes competencias generales de\n",
    "máster:\n",
    "- Capacidad para proyectar, calcular y diseñar productos, procesos e\n",
    "instalaciones en todos los ámbitos de la ingeniería informática.\n",
    "- Capacidad para el modelado matemático, cálculo y simulación en\n",
    "centros tecnológicos y de ingeniería de empresa, particularmente en\n",
    "tareas de investigación, desarrollo e innovación en todos los ámbitos\n",
    "relacionados con la ingeniería informática.\n",
    "- Capacidad para aplicar los conocimientos adquiridos y solucionar\n",
    "problemas en entornos nuevos o poco conocidos dentro de contextos\n",
    "más amplios y multidisciplinares, siendo capaces de integrar estos\n",
    "conocimientos.\n",
    "- Poseer habilidades para el aprendizaje continuo, autodirigido y\n",
    "autónomo.\n",
    "- Capacidad para modelar, diseñar, definir la arquitectura, implantar,\n",
    "gestionar, operar, administrar y mantener aplicaciones, redes,\n",
    "sistemas, servicios y contenidos informáticos.\n",
    "\n",
    "Las competencias específicas de esta asignatura que se trabajan en esta\n",
    "prueba son:\n",
    "- Entender qué es el aprendizaje automático en el contexto de la\n",
    "inteligencia artificial.\n",
    "- Distinguir entre los diferentes tipos y métodos de aprendizaje.\n",
    "- Aplicar las técnicas estudiadas a un caso real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuNVYxUITovf"
   },
   "source": [
    "### Objectivos\n",
    "\n",
    "En esta PEC aprenderéis como funcionan los sistemas recomendadores. Más concretamente, aprenderéis a trabajar con funciones de similitud y a hacer predicciones con una biblioteca de uso habitual en el área: Surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MVVhDnHT_-Q"
   },
   "source": [
    "### Recursos\n",
    "\n",
    "Esta PEC requiere los recursos siguientes:\n",
    "\n",
    "Archivos proporcionados:\n",
    "\n",
    "  * El archivo .ipynb que estáis leyendo ahora mismo y que deberéis completar para resolver la PEC.\n",
    "  * El archivo DATASET.CSV, el cual contiene los datos con los que vais a trabajar.\n",
    "\n",
    "Complementarios: \n",
    "  * Manual de teoría de la asignatura.\n",
    "  * Aocumentación de las librerías utilizadas, entre las que destacan Pandas, NumPy y surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm4AkmzXUWPx"
   },
   "source": [
    "### Entrega y criterios de evaluación\n",
    "\n",
    "La PEC se debe entregar el **29 de marzo del 2022**.\n",
    "\n",
    "La entrega debe incluir una versión editada de este cuaderno (.ipynb). Se recomienda el uso de Google Colab (https://colab.research.google.com/). El código de las soluciones a los ejercicios se debe implementar y ejecutar en las celdas de código proporcionadas y la respuestas justificadas se deben agregar a las celdas de texto correspondientes.\n",
    "\n",
    "Todas las respuestas deben estar correctamente razonadas y justificadas. **Las soluciones que no vayan acompañadas de la correspondiente respuesta razonada no serán evaluadas**.\n",
    "\n",
    "Junto a cada una de las actividades se indica su puntuación. La evaluación de cada apartado considerará la respuesta proporcionada, la corrección y la optimización del código y también la claridad, tanto del código como de las explicaciones que se soliciten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxm-7MuJU6bK"
   },
   "source": [
    "### Descripción de la PEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripción de los datos\n",
    "\n",
    "Los datos con los que trabajaréis provienen del \"User-Book Interactions dataset\". Se trata de un conjunto de datos (o *dataset*) que recoge valoraciones de libros hechas por distintos usuarios. Encontraréis información detallada en su sitio web:\n",
    "\n",
    "https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/shelves?authuser=0\n",
    "\n",
    "Este conjunto de datos se ha utilizado en las siguientes publicaciones:\n",
    "\n",
    "* Mengting Wan, Julian McAuley, *Item Recommendation on Monotonic Behavior Chains*, in RecSys'18.\n",
    "\n",
    "* Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley, *Fine-Grained Spoiler Detection from Large-Scale Review Corpora*, in ACL'19.\n",
    "\n",
    "Sin embargo no debéis trabajar con el dataset antrior sino con el que os proporcionamos (DATASET.CSV), el cual es una versión reducida del original.\n",
    "\n",
    "El formato de DATASET.CSV es simple.\n",
    "\n",
    "* La primera fila es la cabecera y contiene los nombres de las tres columnas: user_id, book_id, rating.\n",
    "* Las siguientes filas contienen los datos. Cada fila se corresponde a una valoración de un libro hecha por un usuario. En particular, en cada fila encontraréis el identificador del usuario que ha hecho la valoración (user_id), el identificador del libro que ha valorado (book_id) y la valoración hecha (rating).\n",
    "\n",
    "Las valoraciones son números entre 0 (el libro no ha gustado) y 5 (el libro ha gustado mucho). Los identificadores son simplemente números que identifican unívocamente cada usuario y cada libro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación del entorno\n",
    "\n",
    "Para realizar esta PEC deberéis programar en Python y utilizar algunas de sus bibliotecas más conocidas, como Pandas o NumPy. Estas dos bibliotecas se encuentran preinstaladas en la mayoría de distribuciones de Python. Si no es vuestro caso, podéis instalarlas ejecutando\n",
    "\n",
    "* !pip install pandas\n",
    "* !pip install numpy\n",
    "\n",
    "Podéis encontrar información detallada sobre estas dos bibliotecas en:\n",
    "\n",
    "* https://pandas.pydata.org/\n",
    "* https://numpy.org/\n",
    "\n",
    "También utilizaréis la biblioteca surprise:\n",
    "\n",
    "* http://surpriselib.com/\n",
    "\n",
    "Esta biblioteca no se encuentra preinstalada en muchas distribuciones de Python. Más adelante se os indicará como instalarla.\n",
    "\n",
    "La entrega de la PEC consistirá en esta plantilla ipynb debidamente completada, tal y como se indica más adelante. Para ello podéis trabajar en local mediante Jupyter Lab o similares o en remoto mediante Google Colab. Nuestra recomendación es que utilicéis Google Colab, accediendo a él mediante el usuario de Google de vuestra cuenta de la UOC, ya que las bibliotecas más relevantes se encuentran preinstaladas en Colab. Algunas, no obstante, no lo están. En particular, Pandas y NumPy ya están preinstaladas en Colab (no es necesario ejecutar las líneas anteriores) mientras que Surprise no lo está (deberéis instalarla como se indicará más adelante).\n",
    "\n",
    "A continuación se proporcionan instrucciones para preparar el entorno de trabajo tanto si es local como si es remoto (Google Colab).\n",
    "\n",
    "Si trabajáis en local (Jupyter Lab o similares) debéis ubicar el archivo DATASET.CSV en la misma carpeta donde se encuentre este ipynb y asignar True a la variable *localDataset*.\n",
    "\n",
    "Si trabajáis en Google Colab deberéis ubicar el archivo DATASET.CSV en el Google Drive de la misma cuenta donde ejecutáis Colab. Lo más recomendable es que lo hagáis todo des de la cuenta de Google de la UOC. Una vez hecho este deberéis asignar False a la variable *localDataset* y modificar la variable *remotePath* (también en la siguiente celda) para que apunte a la carpeta de Google Drive donde habéis guardado DATASET.CSV.\n",
    "\n",
    "En cualquiera de los dos casos debéis ejecutar la siguiente celda tras seguir las instrucciones anteriores. También en los dos casos, la siguiente celda almacenará la ruta a DATASET.CSV dentro de la variable *localFileName*. Por lo tanto, cuando tengáis que cargar el dataset deberéis hacerlo utilizando *localFileName* como nombre de archivo.\n",
    "\n",
    "Notad que si trabajáis en Colab el script de la siguiente celda monta vuestra unidad de Google Drive dentro del espacio de Colab. Por este motivo se os solicitará que autoricéis a Colab el acceso a vuestro Google Drive.\n",
    "\n",
    "Notad también que deberéis ejecutar la siguiente celda cada vez que empecéis a trabajar en este ipynb, tanto si trabajáis en local como si lo hacéis en Colab. El propio archivo ipynb se guarda automáticamente en vuestro espacio de Google y podéis también guardarlo en local para hacer la entrega de la PEC. Ahora bien, los archivos que guardéis en vuestro espacio Colab **se pierden al cerrar la sesión**.\n",
    "\n",
    "La realización de la PEC no requiere que se guarde ninún archivo. Ahora bien, si por algún motivo lo hacéis recordad que para conservarlo deberéis copiarlo desde Colab a vuestro Drive (o bajarlo a vuestro ordenador) y, probablemente, copiarlo de nuevo a vuestro espacio Colab al volver a acceder a él. \n",
    "\n",
    "Encontraréis información detallada sobre estos aspectos en el siguiente link:\n",
    "\n",
    "https://colab.research.google.com/notebooks/io.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WORKING LOCALLY ]\n",
      "[ DONE ]\n"
     ]
    }
   ],
   "source": [
    "# FOLLOW THE PREVIOUS INSTRUCTIONS AND RUN THE CELL EVERY TIME YOU START WORKING\n",
    "\n",
    "localDataset=True\n",
    "\n",
    "if not localDataset:\n",
    "    from google.colab import drive\n",
    "    import shutil\n",
    "    import os\n",
    "    mountPoint='/content/drive'\n",
    "    remotePath='MyDrive/PAC1_IAA_2122'\n",
    "    localPath='.'\n",
    "    fileName='DATASET.CSV'\n",
    "    remoteFileName=os.path.join(mountPoint,remotePath,fileName)\n",
    "    localFileName=os.path.join(localPath,fileName)\n",
    "\n",
    "    print('[ COPYING DATA FROM GOOGLE DRIVE TO LOCAL COLAB SPACE ]')\n",
    "    if os.path.isfile(localFileName):\n",
    "      print('  * FILE %s ALREADY PRESENT.'%localFileName)\n",
    "    else:\n",
    "      if os.path.isdir(mountPoint):\n",
    "        print('  * GOOGLE DRIVE ALREADY MOUNTED AT %s'%mountPoint)\n",
    "      else:\n",
    "        print('  * MOUNTING GOOGLE DRIVE AT %s'%mountPoint)\n",
    "        drive.mount(mountPoint)\n",
    "      print('  * COPYING FROM %s TO %s'%(remoteFileName,localFileName))\n",
    "      shutil.copy(remoteFileName,localFileName)\n",
    "else:\n",
    "    print('[ WORKING LOCALLY ]')\n",
    "    localFileName='DATASET.CSV'\n",
    "    \n",
    "print('[ DONE ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGkmLXEYU-bp"
   },
   "source": [
    "### Ejercicios\n",
    "\n",
    "Cada Actividad es independiente de las otras. Por lo tanto, si bien es recomendable realizarlas en el orden propuesto, es posible resolverlas en cualquier orden.\n",
    "\n",
    "Notad que el comentario anterior se refiere a las Actividades. Los apartados dentro de cada Actividad sí deben realizarse en el orden propuesto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Actividad 1: Comprender los datos**\n",
    "\n",
    "Para realizar esta actividad utilizaréis bibliotecas como Pandas y NumPy. Por lo tanto, es importante que os documentéis sobre su funcionamiento. En particular, Pandas permite filtrar o pivotar datos cómodamente y NumPy permite realizar de forma eficiente cálculos y búsquedas de datos en matrices numéricas. En este sentido os pueden ser útiles funciones de NumPy como argmax y argmin. Tened en cuenta también que es posible convertir datos desde un dataframe de Pandas a una matriz NumPy mediante las funciones existentes en dichas bibliotecas.\n",
    "\n",
    "**a) (0.25 puntos)** Cargad el dataset como un dataframe de Pandas denominado dataSet. Aunque en futuras acctividades podéis crear (si lo necesitáis por algú motivo) versiones alternativas, filtradas, modificadas, procesadas, ... del dataset, la variable dataSet no debe modificarse y siempre debe contener los datos tal y como se carguen en este apartado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataSet = pd.read_csv(\"DATASET.CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) (0.75 puntos)** Completa la siguiente celda de manera que se ponga dentro de las variables indicadas la información que se lista a continuación. El tipo de las variables debe coincidir con el del formato utilizado en los \"print\" de forma que los resuldatos se visualicen correctamente.\n",
    "\n",
    "* numEntries debe contener el número de registros (filas) del dataset.\n",
    "* numAttributes debe contener el número de atributos (columnas) del dataset.\n",
    "* numUsers debe contener el número de usuarios distintos que aparecen en el dataset.\n",
    "* numBooks debe contener el número de libros distintos que se han valorado en el dataset.\n",
    "* meanRate debe contener la valoración media del dataset.\n",
    "* exampleRate debe contener la valoración hecha por el usuario con identificador 361926 del libro con identificador 301,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRIES         : 9835\n",
      "ATTRIBUTES      : 3\n",
      "DIFFERENT USERS : 2955\n",
      "DIFFERENT BOOKS : 513\n",
      "AVERAGE RATING  : 3.844\n",
      "EXAMPLE RATING  : 5\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE AND EXECUTE THE CELL\n",
    "\n",
    "numEntries=len(dataSet)\n",
    "numAttributes=len(dataSet.columns)\n",
    "numUsers=len(pd.unique(dataSet['user_id']))\n",
    "numBooks=len(pd.unique(dataSet['book_id']))\n",
    "meanRate=dataSet['rating'].mean()\n",
    "exampleRate=dataSet.loc[(dataSet['user_id'] == 361926) & (dataSet['book_id'] == 301), 'rating'].iat[0]\n",
    "\n",
    "\n",
    "# DO NOT MODIFY THE FOLLOWING LINES\n",
    "\n",
    "print('ENTRIES         : %d'%numEntries)\n",
    "print('ATTRIBUTES      : %d'%numAttributes)\n",
    "print('DIFFERENT USERS : %d'%numUsers)\n",
    "print('DIFFERENT BOOKS : %d'%numBooks)\n",
    "print('AVERAGE RATING  : %.3f'%meanRate)\n",
    "print('EXAMPLE RATING  : %d'%exampleRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) (0.75 puntos)** Completa la siguiente celda de manera que cada una de las variables indicadas contenga el valor que se indica a continuación. El tipo de cada variable debe coincidir con el del formato utilizado en los \"print\" de forma que los resultados se visualicen correctamente. Dentro de la celda de la respuesta podéis incluir funciones o código auxiliar si lo necesitáis. Si hay varios valores posibles para una variable, asignadle uno de ellos.\n",
    "\n",
    "* mostProlificUser debe contener el identificador del usuario que más libros ha valorado.\n",
    "* maxVotes debe contener el número de libros que ha valorado el usuario mostProlificUser.\n",
    "* leastProlificUser debe contener el identificador del usuario que menos libros ha valorado.\n",
    "* minVotes debe contener el número de libros que ha valorado el usuario leastProlificUser.\n",
    "* meanVotes debe contener el número promedio de valoraciones hechas por los usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST PROLIFIC USER      : 88757\n",
      "MAX. NUMBER OF VOTES    : 18\n",
      "LEAST PROLIFIC USER     : 38186\n",
      "MIN. NUMBER OF VOTES    : 1\n",
      "AVERAGE NUMBER OF VOTES : 5.427\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE AND EXECUTE THE CELL\n",
    "\n",
    "mostProlificUser=dataSet['user_id'].mode().iat[0]\n",
    "maxVotes=dataSet.loc[dataSet['user_id'] == mostProlificUser].count()['book_id']\n",
    "leastProlificUser=dataSet['user_id'].value_counts().index[-1]\n",
    "minVotes=dataSet.loc[dataSet['user_id'] == leastProlificUser].count()['book_id']\n",
    "meanVotes=dataSet.user_id.map(dataSet.user_id.value_counts()).mean()\n",
    "\n",
    "# DO NOT MODIFY THE FOLLOWING LINES\n",
    "\n",
    "print('MOST PROLIFIC USER      : %d'%mostProlificUser)\n",
    "print('MAX. NUMBER OF VOTES    : %d'%maxVotes)\n",
    "print('LEAST PROLIFIC USER     : %d'%leastProlificUser)\n",
    "print('MIN. NUMBER OF VOTES    : %d'%minVotes)\n",
    "print('AVERAGE NUMBER OF VOTES : %.3f'%meanVotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) (0.75 puntos)** Completa la siguiente celda de manera que cada una de las variables indicadas contenga el valor que se indica a continuación. El tipo de cada variable debe coincidir con el del formato utilizado en los \"print\" de forma que los resultados se visualicen correctamente. Dentro de la celda de la respuesta podéis incluir funciones o código auxiliar si lo necesitáis. Si hay varios valores posibles para una variable, asignadle uno de ellos.\n",
    "\n",
    "* mostReadBook debe contener el identificador del libro que más usuarios han valorado.\n",
    "* maxUsers debe contener el número de usuarios que han valorado el libro mostReadBook.\n",
    "* leastReadBook debe contener el identificador del libro que menos usuarios han valorado.\n",
    "* minUsers debe contener el número de usuarios que han valorado el libro leastReadBook.\n",
    "* meanUsers debe contener el número medio de usuarios que han valorado cada libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST READ BOOK          : 536\n",
      "MAX. USERS              : 85\n",
      "LEAST READ BOOK         : 23819\n",
      "MIN. USERS              : 1\n",
      "AVERAGE NUMBER OF USERS : 43.000\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE THE CELL\n",
    "\n",
    "mostReadBook=dataSet.book_id.mode().iat[0]\n",
    "maxUsers=dataSet.loc[dataSet['book_id'] == mostReadBook]['user_id'].count()\n",
    "leastReadBook=dataSet.book_id.value_counts().index[-1]\n",
    "minUsers=dataSet.loc[dataSet['book_id'] == leastReadBook]['user_id'].count()\n",
    "meanUsers=(maxUsers+minUsers)/2\n",
    "\n",
    "# DO NOT MODIFY THE FOLLOWING LINES\n",
    "\n",
    "print('MOST READ BOOK          : %d'%mostReadBook)\n",
    "print('MAX. USERS              : %d'%maxUsers)\n",
    "print('LEAST READ BOOK         : %d'%leastReadBook)\n",
    "print('MIN. USERS              : %d'%minUsers)\n",
    "print('AVERAGE NUMBER OF USERS : %.3f'%meanUsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) (0.75 puntos)** Sabiendo que queremos construir un recomendador de libros y basándoos en los resultados de los apartados anteriores, creéis que la distribución de los datos es adecuada? Justificad la respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir un recomendador de libros basándonos en técnicas de _Machine Learning_ la distribución de datos es adecuada ya que está dividida en 3 columnas \"user_id\", \"book_id\" y \"rating\" y este es el formato esperado en algunas librerías como surprise. Sin embargo, podrían existir más puntuaciones de los usuarios frente a muchos más libros, teniendo en cuenta el volumen del dataset vemos que el usuario con mayor número de votos ha emitido solo 18 y que la media de votos es de 5, muy poca dado el volumen de libros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Actividad 2: Similitudes**\n",
    "\n",
    "**a) (1.25 puntos)** Programad la función *compute_similarity* que recibe como parámetros de entrada dos identificadores de usuario (*firstUser* y *secondUser*) y retorna la similitud Euclídea entre los dos (*theSimilarity*) y también el número de libros que han valorado en común los dos usuarios (*numCommon*). Para calcular la similitud Euclidea debéis:\n",
    "\n",
    "* Utilizar sólo  los libros valorados por los dos usuarios.\n",
    "* Calcularla a partir de la distancia Euclidea como s=1/(1+d), donde s es la similitud y d es la distancia Euclidea.\n",
    "\n",
    "Para programar la función tan solo podéis utilizar las bibliotecas Pandas y NumPy. La función puede suponer que *dataSet* es una variable global. Además, si debéis precalcular algún dato para acelerar los cálculos, hacedlo también en una variable global.\n",
    "\n",
    "Utilizad la definición que ya se os proporciona, manteniendo el \"return\" como su última línea. El código que se proporciona a continuación de la función no debe modificarse y debe mostrar los resultados correctamente al ejecutar la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users 19948 and 433738 rated 4 books in common, resulting in a similarity of 0.205\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_similarity(firstUser,secondUser):\n",
    "    df = pd.merge(dataSet.loc[dataSet.user_id == firstUser], dataSet.loc[dataSet.user_id == secondUser], on='book_id')    \n",
    "    dist = np.sqrt(np.sum(np.square(np.array(df['rating_x']) - np.array(df['rating_y']))))\n",
    "    simil = 1 / (1 + dist)\n",
    "    return simil, len(df['rating_x'])\n",
    "\n",
    "# DO NOT MODIFY THE FOLLOWING LINES\n",
    "\n",
    "# As an example, compute the similarity and the number of common books\n",
    "# between users 19948 and 433738\n",
    "firstUser=19948\n",
    "secondUser=433738\n",
    "\n",
    "theSimilarity,numCommon=compute_similarity(firstUser,secondUser)\n",
    "print('Users %d and %d rated %d books in common, resulting in a similarity of %.3f'%(firstUser,secondUser,numCommon,theSimilarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) (1.25 puntos)** Programad la función *compute_similarity_matrix* la cual debe construir dos matrices:\n",
    "\n",
    "* *similarityMatrix* debe ser la matriz de simitudes Euclideas. La matriz debe contener todas las similitudes Euclideas entre todas las posibles parejas de usuarios de forma que similarityMatrix[r,c] debe contener la similitud Euclidea entre el usuario con identificador allUsers[r] y el usuario con identificador allUsers[c]. Aunque, por definición, similarityMatrix[x,x]=1 para todos los posibles valores de x, asignad un 0 a estas posiciones (la diagonal) para prevenir problemas en actividades posteriores.\n",
    "* *commonMatrix* debe contener en cada celda [r,c] el número de libros en común que han valorado los usuarios con identificador allUsers[r] y allUsers[c].\n",
    "\n",
    "Ambas matrices serán cuadradas y simétricas y tendrán tantas filas (y columnas) como usuarios distintos existan en el dataset. Ya se or proporciona la línea de código que costruye el vector allUsers. La función no tiene parámetros de entrada pero puede utilizar funciones y datos globales ya definidos o calculados anteriormente. Como en actividades anteriores, respetad la cabecera de la función y el \"return\" proporcionados.\n",
    "\n",
    "Notad que el tiempo que tarde en ejecutarse la función puede depender en gran medida de la optimización de funciones anteriores. Si el código (de esta función y de las anteriores) está poco optimizado, el tiempo de ejecución puede ser muy alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE AND EXECUTE THE CELL\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get all the user ids\n",
    "allUsers=np.array(dataSet['user_id'].unique())\n",
    "\n",
    "\n",
    "# The requested function\n",
    "def compute_similarity_matrix():\n",
    "    similarityMatrix = []\n",
    "    commonMatrix = []\n",
    "    for user_1 in allUsers:\n",
    "        simils = []\n",
    "        commons = []\n",
    "        for user_2 in allUsers:\n",
    "            if user_1 == user_2:\n",
    "                simil = 0\n",
    "            else:\n",
    "                simil, common = compute_similarity(user_1, user_2)\n",
    "                commons.append(common)\n",
    "            simils.append(simil)\n",
    "        similarityMatrix.append(simils)\n",
    "        commonMatrix.append(commons)\n",
    "    return similarityMatrix,commonMatrix\n",
    "\n",
    "# DO NOT MODIFY THE FOLLOWING LINES\n",
    "\n",
    "similarityMatrix,commonMatrix=compute_similarity_matrix()\n",
    "\n",
    "# As an example, get the similarity and common reviews between users 19948 and 433738\n",
    "firstUser=19948\n",
    "secondUser=433738\n",
    "idxFirst=int(np.argwhere(allUsers==firstUser))\n",
    "idxSecond=int(np.argwhere(allUsers==secondUser))\n",
    "theSimilarity=similarityMatrix[idxFirst,idxSecond]\n",
    "numCommon=commonMatrix[idxFirst,idxSecond]\n",
    "print('Users %d and %d rated %d books in common, resulting in a similarity of %.3f'%(firstUser,secondUser,numCommon,theSimilarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) (1.25 puntos)** Utilizando la matriz *similarityMatrix* construida en la celda anterior (y que podéis considerar una variable global), programad la función *recommend_books*. Esta función debe recibir como parámetro de entrada un identificador de usuario (*theUser*) y debe retornar un array (*theBooks*) que contenga los identificadores de los libros que podrían recomendarse a ese usuario. Como de costumbre, mantened el formato de definició y retorno de la función que se os proporciona y aseguraos que el código de ejemplo suministrado funciona. Si es necesario podéis definir funciones auxiliares y acceder a variables globales anteriores.\n",
    "\n",
    "Tras la celda con el código encontraréis una celda de texto (*markdown*). En dicha celda deberéis explicar claramente los criterios que habéis seguido ara determinar los libros recomendados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE AND EXECUTE THE FOLLOWING CELL\n",
    "\n",
    "def recommend_books(theUser):\n",
    "    \n",
    "    return recommendedBooks\n",
    "\n",
    "# DO NOT MODIFY THE FOLLOWING LINES\n",
    "\n",
    "theUser=19948\n",
    "recommendedBooks=recommend_books(theUser)\n",
    "print('The recommended books for user %d are %s '%(theUser,','.join([str(i) for i in recommendedBooks])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicad a continuación los criterios seguidos para determinar las recomendaciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Actividad 3: Surprise**\n",
    "\n",
    "En esta actividad trabajaréis con la biblioteca surprise. En caso de no tenerla ya instalada, ejecutad la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) (1 punto)** Haced un programa que entrene los algoritmos de Surprise SVD, KNNBasic, KNNWithMeans y NormalPredictor utilizando todos los datos de DATASET.CSV. Una vez entrenados, utilizad cda uno de los algoritmos para predecir la puntuación que daría el usuario con identificador 19948 al libro con identificador 536. Comparad esta predicción con la puntuacion real. ¿Qué algoritmo se acerca más a la puntuacón real? ¿A qué creéis que se debe? Como de costumbre, el programa que desarrolléis debe proporcionar resultados compatibles con el código de ejemplo que se os proporciona ya dentro de la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "{'KNNBasic': 4.083333333333333,\n",
      " 'KNNWithMeans': 3.731020522687189,\n",
      " 'NormalPredictor': 3.840953144788036,\n",
      " 'SVD': 3.9583937154390134}\n",
      "closest estimate was SVD with 0.041606284560986584\n"
     ]
    }
   ],
   "source": [
    "import surprise\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "real_estimate = exampleRate=dataSet.loc[(dataSet['user_id'] == 19948) & (dataSet['book_id'] == 536), 'rating'].iat[0]\n",
    "closest_estimate = None\n",
    "\n",
    "reader = surprise.dataset.Reader(sep=',', skip_lines=1)\n",
    "data = surprise.dataset.Dataset.load_from_file('DATASET.CSV', reader=reader)\n",
    "\n",
    "training = data.build_full_trainset()\n",
    "\n",
    "method_estimate = {}\n",
    "algorithms = [surprise.SVD(), surprise.KNNBasic(), surprise.KNNWithMeans(), surprise.NormalPredictor()]\n",
    "for method in algorithms:\n",
    "    method.fit(training)\n",
    "    prediction = method.predict(\"19948\", \"536\")\n",
    "    result = prediction.est\n",
    "    difference = abs(result - real_estimate)\n",
    "    method_name = method.__class__.__name__\n",
    "    method_estimate[method_name] = result\n",
    "    if not closest_estimate or difference < closest_estimate[1]:\n",
    "        closest_estimate = (method_name, difference)\n",
    "\n",
    "pprint(method_estimate)\n",
    "print(f\"closest estimate was {closest_estimate[0]} with {closest_estimate[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo que más se ha acercado a la puntuación real es SVD (Single Value Decomposition) que nos permite hacer una descomposición de matrices de tal forma que podemos calcular sus _eigenvectors_ y así hacer predicciones sobre la misma aunque estos valores no estén definidos en el conjuntos de datos.\n",
    "Sin embargo, en otras iteraciones del código se ha visto como ganador al algoritmo KNNBasic con puntuaciones muy reñinas a SVD. KNNBasic busca los k usuarios con los ratings más similares para dar una recomendación y, en este caso, como los ratings están normalizados se les puede dar más peso a estos para determinar la similitud entre usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) (1 punto)** Haced un programa que, para los mismos algoritmos del apartado anterior, realice una validación cruzada utilizando 5 folds y obteniendo el RMSE. ¿Qué algoritmo arroja mejores resultados? ¿Coincide con el que mejores resultados proporcionó en el apartado anterior? Justificad brevemente la respuesta. Como de costumbre, el programa debe proporcionar resultados compatibles con el código de ejemplo que se incluye dentro de la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.0739  1.1262  1.0850  1.0393  1.0780  1.0805  0.0278  \n",
      "Fit time          0.65    0.59    0.56    0.55    0.57    0.58    0.04    \n",
      "Test time         0.02    0.02    0.01    0.01    0.01    0.02    0.00    \n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.2280  1.2683  1.2533  1.2412  1.2430  1.2468  0.0134  \n",
      "Fit time          0.31    0.32    0.34    0.44    0.50    0.38    0.08    \n",
      "Test time         0.07    0.07    0.09    0.15    0.07    0.09    0.03    \n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.2282  1.2966  1.2906  1.2585  1.2596  1.2667  0.0247  \n",
      "Fit time          0.37    0.36    0.61    0.40    0.49    0.45    0.09    \n",
      "Test time         0.06    0.11    0.08    0.12    0.06    0.09    0.03    \n",
      "Evaluating RMSE of algorithm NormalPredictor on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.5023  1.5176  1.4999  1.5059  1.4998  1.5051  0.0066  \n",
      "Fit time          0.02    0.02    0.02    0.01    0.01    0.02    0.00    \n",
      "Test time         0.04    0.03    0.02    0.02    0.02    0.03    0.01    \n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "\n",
    "folds = 5\n",
    "for method in algorithms:\n",
    "    cross_validate(method, data, measures=['RMSE'], cv=folds, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo que arroja mejores resultados en la validación cruzada ha sido SVD con una media de RMSE de 1.0805 y  coincide con el algoritmo seleccionado en el apartado anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Actividad 4: Tendencias actuales**\n",
    "\n",
    "**a) (1 punto)** Se puede decir que en el mundo del aprendizaje computacional ha habido un antes y un después de la popularización de las redes neuronales y el denominado *deep learning*. El campo de los recomendadores no es una excepción.\n",
    "\n",
    "Buscad información sobre sistemas recomendadores basados en redes neuronales. Describid muy brevemente cuál es la base de su funcionamiento y qué ventajas e inconvenientes presentan respecto a métodos clásicos como los utilizados en esta PEC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los sistemas recomendadores tradicionales están basados en el contenido colaborativo y basan sus recomendaciones en  datos históricos para construir modelos que se ajusten a un usuario y a sus preferencias y clasificarlo correctamente junto a otros usuarios. Estos sistemas de _Machine Learning_ analizan los datos de manera lineal y requieren un conjunto menor de datos que en sistemas basados en _Deep Learning_, sin embargo, también son dependientes de un cierto volumen de estos si quieren una predicción más precisa y relevante a un nuevo usuario.\n",
    "El Deep Learning es un subcampo de _Machine Learning_ que consiste en algoritmos inspirados en el funcionamiento del cerebro humano; concretamente, son familias de algoritmos que hacen una abstracción de cómo funciona una red neuronal humana. Los sistemas recomendadores basados en _Deep Learning_ permiten el procesamiento de datos de manera no lineal, son más efectivos en la minería de datos valiosos y pueden dar con características especiales en los conjuntos de datos de manera automática, sin embargo, estos modelos necesitan altas prestaciones ya que requieren de grandes volúmenes de datos para su entrenamiento y las características encontradas son difíciles de predecir o de justificar ya que se encuentran en un modelo de caja negra.\n",
    "\n",
    "**Referencias**\n",
    "* [The rise of deep learning recommender systems](https://www.dynamicyield.com/lesson/deep-learning-recommendations/)  \n",
    "* [Deep Learning Based Recommender Systems](https://medium.com/sciforce/deep-learning-based-recommender-systems-b61a5ddd5456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlnkRb4CiDsR"
   },
   "source": [
    "### Nota: Propiedad intelectual\n",
    "\n",
    "A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por tanto comprensible hacerlo en el marco de una práctica de los estudios del Grado Multimedia, siempre y cuando esto se documente claramente y no suponga plagio en la práctica.\n",
    "\n",
    "Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se presentará junto con ella un documento en el que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y el su estatus legal: si la obra está protegida por copyright o se acoge a alguna otra licencia de uso (Creative Commons, GNU, GPL ...). El estudiante deberá asegurarse de que la licencia que sea no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente deberá asumir que la obra está protegida por copyright.\n",
    "\n",
    "Deberán, además, adjuntar los archivos originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.\n",
    "\n",
    "Otro punto a considerar es que cualquier práctica que haga uso de recursos protegidos por copyright no podrá en ningún caso publicarse en Mosaic, la revista del Grado en Multimedia en la UOC, a no ser que los propietarios de los derechos intelectuales den su autorización explícita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Plantilla PAC - castellà.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
